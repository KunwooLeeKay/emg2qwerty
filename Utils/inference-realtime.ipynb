{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "project_path = os.path.expanduser('~/emg2qwerty')\n",
    "sys.path.append(project_path)\n",
    "\n",
    "from emg2qwerty.data import EMGSessionData, LabelData, WindowedEMGDataset\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First import test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import test data\n",
    "import yaml\n",
    "user_name = 'user0'\n",
    "yaml_path = os.path.expanduser(f'~/emg2qwerty/config/user/{user_name}.yaml')\n",
    "yaml = yaml.safe_load(open(yaml_path, 'r'))\n",
    "session_list = yaml['dataset']['test']\n",
    "\n",
    "# Later, we will need to iterate\n",
    "# for session in session_list:\n",
    "#     filename = session['session']\n",
    "filename = session_list[0]['session'] + '.hdf5'\n",
    "\n",
    "hdf5_dir = os.path.join(project_path, 'data') \n",
    "hdf5_path = os.path.join(hdf5_dir, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What should our stride & window size be?\n",
    "- Let's assume we are keeping window size <- the legnth of prediction (4 seconds)\n",
    "- Let's assume we are also using the same padding, 1000 ms in the past. This part is given as input, but keystrokes inside this padding is not predicted\n",
    "- What should our stride be?\n",
    "\n",
    "## Note) Later Steps\n",
    "- window size should decrease\n",
    "- window size + padding should remain the same. (increase padding, reduce window size)\n",
    "- decrease stride to an appropriate sampling rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1)\n",
    "- How long does it take for the baseline model to produce inference for one window?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TDSConvCTCModule(\n",
       "  (model): Sequential(\n",
       "    (0): SpectrogramNorm(\n",
       "      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): MultiBandRotationInvariantMLP(\n",
       "      (mlps): ModuleList(\n",
       "        (0-1): 2 x RotationInvariantMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=528, out_features=384, bias=True)\n",
       "            (1): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Flatten(start_dim=2, end_dim=-1)\n",
       "    (3): TDSConvEncoder(\n",
       "      (tds_conv_blocks): Sequential(\n",
       "        (0): TDSConv2dBlock(\n",
       "          (conv2d): Conv2d(24, 24, kernel_size=(1, 32), stride=(1, 1))\n",
       "          (relu): ReLU()\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TDSFullyConnectedBlock(\n",
       "          (fc_block): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TDSConv2dBlock(\n",
       "          (conv2d): Conv2d(24, 24, kernel_size=(1, 32), stride=(1, 1))\n",
       "          (relu): ReLU()\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TDSFullyConnectedBlock(\n",
       "          (fc_block): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TDSConv2dBlock(\n",
       "          (conv2d): Conv2d(24, 24, kernel_size=(1, 32), stride=(1, 1))\n",
       "          (relu): ReLU()\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TDSFullyConnectedBlock(\n",
       "          (fc_block): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): TDSConv2dBlock(\n",
       "          (conv2d): Conv2d(24, 24, kernel_size=(1, 32), stride=(1, 1))\n",
       "          (relu): ReLU()\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): TDSFullyConnectedBlock(\n",
       "          (fc_block): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): Linear(in_features=768, out_features=99, bias=True)\n",
       "    (5): LogSoftmax(dim=-1)\n",
       "  )\n",
       "  (ctc_loss): CTCLoss()\n",
       "  (metrics): ModuleDict(\n",
       "    (train_metrics): MetricCollection(\n",
       "      (CharacterErrorRates): CharacterErrorRates(),\n",
       "      prefix=train/\n",
       "    )\n",
       "    (val_metrics): MetricCollection(\n",
       "      (CharacterErrorRates): CharacterErrorRates(),\n",
       "      prefix=val/\n",
       "    )\n",
       "    (test_metrics): MetricCollection(\n",
       "      (CharacterErrorRates): CharacterErrorRates(),\n",
       "      prefix=test/\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the personalized model for the user\n",
    "from emg2qwerty.lightning import TDSConvCTCModule\n",
    "model_path = os.path.expanduser(f'~/emg2qwerty/models/personalized-finetuned/{user_name}.ckpt')\n",
    "model = TDSConvCTCModule.load_from_checkpoint(model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small window version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch :23641, input shape: torch.Size([622, 2, 16, 33]), label shape : torch.Size([0])\n"
     ]
    }
   ],
   "source": [
    "from emg2qwerty import transforms\n",
    "# test_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# window_data = WindowedEMGDataset(hdf5_path, window_length=8000, padding=(2000, 0), transform = test_transforms)\n",
    "# print(f'batch :{window_data.__len__()}, input shape: {window_data[0][0].shape}, label shape : {window_data[0][1].shape}')\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.ToTensor(), transforms.LogSpectrogram()])\n",
    "\n",
    "window_data = WindowedEMGDataset(hdf5_path, window_length=100, padding=(9900, 0), transform = test_transforms)\n",
    "print(f'batch :{window_data.__len__()}, input shape: {window_data[2][0].shape}, label shape : {window_data[0][1].shape}')\n",
    "# 622 ~= (8000 + 2000) / 16 = 625 (note 16 = hop length)\n",
    "# Anyway, one window is size [622, 2, 16, 33] after transformation (except the first one <- no padding (no past info))\n",
    "\n",
    "one_frame = window_data[2][0]; one_label = window_data[2][1]\n",
    "one_frame = one_frame.unsqueeze(1); one_label = one_label.unsqueeze(1)\n",
    "\n",
    "one_frame.shape, one_label.shape\n",
    "\n",
    "input_formatted = {'inputs': one_frame, 'targets': one_label,\n",
    " 'input_lengths': torch.tensor([622], dtype=torch.int32),\n",
    " 'target_lengths': torch.tensor([16], dtype=torch.int32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_data.__len__()\n",
    "# window_data[10][0].shape\n",
    "window_data[2009][1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8015735374984138"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's count how many inputs will have label that is not empty\n",
    "# If most datapoints are empty, we should not use them\n",
    "empty_count = 0 \n",
    "for i in range(window_data.__len__()):\n",
    "    if window_data[i][1].shape[0] == 0:\n",
    "        empty_count += 1\n",
    "\n",
    "empty_count / window_data.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only past data version (~30ms before keystroke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emg2qwerty import transforms\n",
    "# test_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# window_data = WindowedEMGDataset(hdf5_path, window_length=8000, padding=(2000, 0), transform = test_transforms)\n",
    "# print(f'batch :{window_data.__len__()}, input shape: {window_data[0][0].shape}, label shape : {window_data[0][1].shape}')\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.ToTensor(), transforms.LogSpectrogram()])\n",
    "window_data = WindowedEMGDataset(hdf5_path, window_length=100, padding=(9960, -60), transform = test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m     target_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(one_label)\n\u001b[1;32m     10\u001b[0m     input_formatted \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m: one_frame, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m'\u001b[39m: one_label,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_lengths\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([input_length], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32),\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_lengths\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([target_length], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32)}\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_formatted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/emg2qwerty/emg2qwerty/lightning.py:346\u001b[0m, in \u001b[0;36mTDSConvCTCModule.test_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtest_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/emg2qwerty/emg2qwerty/lightning.py:260\u001b[0m, in \u001b[0;36mTDSConvCTCModule._step\u001b[0;34m(self, phase, batch, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m target_lengths \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_lengths\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    257\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_lengths)  \u001b[38;5;66;03m# batch_size\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m emissions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Shrink input lengths by an amount equivalent to the conv encoder's\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# temporal receptive field to compute output activation lengths for CTCLoss.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# NOTE: This assumes the encoder doesn't perform any temporal downsampling\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# such as by striding.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m T_diff \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m emissions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/emg2qwerty/emg2qwerty/lightning.py:248\u001b[0m, in \u001b[0;36mTDSConvCTCModule.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/emg2qwerty/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/emg2qwerty/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/emg2qwerty/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/emg2qwerty/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/emg2qwerty/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/emg2qwerty/emg2qwerty/modules.py:36\u001b[0m, in \u001b[0;36mSpectrogramNorm.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 36\u001b[0m     T, N, bands, C, freq \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# (T, N, bands=2, C=16, freq)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels \u001b[38;5;241m==\u001b[39m bands \u001b[38;5;241m*\u001b[39m C\n\u001b[1;32m     39\u001b[0m     x \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mmovedim(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (N, bands=2, C=16, freq, T)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 3)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(window_data.__len__()):\n",
    "    one_frame = window_data[i][0]; one_label = window_data[i][1]\n",
    "    one_frame = one_frame.unsqueeze(1); one_label = one_label.unsqueeze(1)\n",
    "    input_length = len(one_frame)\n",
    "    target_length = len(one_label)\n",
    "\n",
    "    input_formatted = {'inputs': one_frame, 'targets': one_label,\n",
    "    'input_lengths': torch.tensor([input_length], dtype=torch.int32),\n",
    "    'target_lengths': torch.tensor([target_length], dtype=torch.int32)}\n",
    "    model.test_step(input_formatted)\n",
    "    end_time = time.time()\n",
    "\n",
    "print(f\"Execution time: {end_time - start_time:.6f} seconds\")\n",
    "print(f\"Average prediction time : {(end_time - start_time)/i:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET FORMULATION\n",
    "- Given execution time ~= 0.04, the highest possible sampling rate is 25Hz.\n",
    "- Stride length minimum = 0.04 seconds which is stride = 80 (baseline is 8000 (stride == window length))\n",
    "- But that is minimum, so let's use stride = 160 as safety, with window size = 160, padding = 9840\n",
    "- So the system will wait input for 0.08 seconds then make prediction\n",
    "\n",
    "## How good is this?\n",
    "- The use will experience 120 ms lag.\n",
    "- How can we reduce this?\n",
    "- EMG precede visible movement by about 30~100 ms\n",
    "- If we augment the dataset to predict keystroke 30 ms before the actual timing and train model on that, would it work?\n",
    "\n",
    "- If the augmentation goes successful, the delay will be ~90ms\n",
    "\n",
    "\n",
    "## DATASET FORMULATION 2\n",
    "- Given execution time ~= 0.04, the highest possible sampling rate is 25Hz.\n",
    "- Stride length minimum = 0.04 seconds which is stride = 80 (baseline is 8000 (stride == window length))\n",
    "- Use near minimum, stride = 100, with window size = 100, padding = 9900\n",
    "- So the system will wait input for 0.05 seconds then make prediction\n",
    "\n",
    "## How good is this?\n",
    "- The use will experience 90 ms lag.\n",
    "- How can we reduce this?\n",
    "- EMG precede visible movement by about 30~100 ms\n",
    "- If we augment the dataset to predict keystroke 30 ms before the actual timing and train model on that, would it work?\n",
    "- If the augmentation goes successful, the delay will be ~60ms\n",
    "\n",
    "\n",
    "## Hardware will introduce another source of delay, magnitude ???\n",
    "## Language model will introduce another source of delay\n",
    "- My bluetooth keyboard had 30 ms delay total\n",
    "- The delay will be noticed if ~100ms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: what happens if I do negative padding, so that I only use past information, ~30ms before keystroke?\n",
    "- 30 ms is kind of arbitrary. If we do a better correlation analysis (?) we may get a better insight into what this number should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch :23641, input shape: torch.Size([3, 2, 16, 33]), label shape : torch.Size([0])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     24\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 26\u001b[0m pred, loss, metrics \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtest_step(\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__str__\u001b[39m())\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01memg2qwerty\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CharacterErrorRates\n",
      "\u001b[0;31mKeyError\u001b[0m: 100"
     ]
    }
   ],
   "source": [
    "# Test the small window version!\n",
    "test_transforms = transforms.Compose([transforms.ToTensor(), transforms.LogSpectrogram()])\n",
    "window_data = WindowedEMGDataset(hdf5_path, window_length=100, padding=(9900, 0), transform = test_transforms)\n",
    "print(f'batch :{window_data.__len__()}, input shape: {window_data[0][0].shape}, label shape : {window_data[0][1].shape}')\n",
    "\n",
    "# DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "test_dataloader = DataLoader(\n",
    "            window_data,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_workers=3,\n",
    "            collate_fn=window_data.collate,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True)\n",
    "\n",
    "\n",
    "batch = next(iter(test_dataloader))\n",
    "\n",
    "del test_dataloader\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "pred, loss, metrics = model.test_step(batch[100])\n",
    "print(pred[0].__str__())\n",
    "\n",
    "from emg2qwerty.metrics import CharacterErrorRates\n",
    "cer = CharacterErrorRates()\n",
    "cer.update(pred[0], GT)\n",
    "cer.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "            window_data,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_workers=3,\n",
    "            collate_fn=window_data.collate,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-0.8206,  0.8627,  1.9767,  ...,  0.2005, -0.4596, -2.1619],\n",
       "           [-1.3654,  1.1617,  2.0713,  ..., -0.0701, -0.5394, -0.7612],\n",
       "           [ 0.4448,  0.9819,  2.0513,  ..., -1.0993, -0.9447, -0.2359],\n",
       "           ...,\n",
       "           [-2.8162,  1.0822,  1.4743,  ..., -0.2251, -0.2153,  0.4482],\n",
       "           [-0.2417,  0.7476,  1.7175,  ..., -1.6421, -1.1337, -1.4545],\n",
       "           [-0.5485,  0.5717,  1.6467,  ...,  0.2941, -1.5640, -0.2922]],\n",
       " \n",
       "          [[ 0.8723,  1.5427,  2.1978,  ...,  0.3729,  0.3015, -0.2534],\n",
       "           [ 1.3964,  1.5111,  2.5604,  ...,  0.0914, -0.6499, -0.0425],\n",
       "           [ 1.8263,  1.9207,  3.1723,  ..., -0.1532,  0.2833, -0.0295],\n",
       "           ...,\n",
       "           [ 0.9804,  1.5796,  1.7248,  ..., -0.5702, -0.0725, -0.6250],\n",
       "           [ 0.5530,  1.5083,  1.7351,  ...,  0.1335,  0.5246,  0.6864],\n",
       "           [ 0.6851,  1.1693,  0.1112,  ..., -0.1514,  0.0616, -0.2400]]],\n",
       " \n",
       " \n",
       "         [[[-0.9315,  0.6049,  1.6967,  ...,  0.2184, -0.6759, -2.1156],\n",
       "           [-2.7896,  1.0655,  2.0463,  ..., -0.5223, -0.8451, -1.6827],\n",
       "           [-0.5261,  0.5820,  2.0416,  ..., -0.2250,  0.0917,  0.1977],\n",
       "           ...,\n",
       "           [-0.4525,  0.2097,  0.4044,  ..., -0.2995,  0.3902,  0.5060],\n",
       "           [-1.3786, -0.0080,  1.3264,  ..., -1.0335, -0.6632, -0.2726],\n",
       "           [-1.5326,  0.3206,  1.1113,  ...,  0.0628,  0.3065,  0.3744]],\n",
       " \n",
       "          [[ 1.6011,  2.3168,  2.8110,  ...,  0.4736, -0.2579, -2.4720],\n",
       "           [ 1.7855,  2.7720,  3.2951,  ...,  0.4155,  0.4019,  0.4663],\n",
       "           [ 2.0746,  3.0738,  3.7058,  ...,  0.1773, -1.3410, -0.0587],\n",
       "           ...,\n",
       "           [ 0.8075,  1.5011,  1.5419,  ..., -0.9816, -0.2351,  0.0533],\n",
       "           [ 0.6555,  1.4428,  0.4759,  ...,  0.4186,  0.6541, -1.0083],\n",
       "           [ 0.8631,  1.6158,  1.8582,  ..., -0.3341, -1.1775, -0.7181]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0485,  1.0947,  1.9177,  ..., -0.4235,  0.3177,  0.1323],\n",
       "           [-0.1990,  1.1139,  1.7593,  ..., -0.8896, -0.7979, -1.4064],\n",
       "           [-0.7053,  0.8103,  1.6698,  ...,  0.4969, -1.0633,  0.1727],\n",
       "           ...,\n",
       "           [-0.7510,  0.3600,  1.1582,  ..., -0.2993,  0.5777, -0.5426],\n",
       "           [-0.5523,  0.7025,  1.9757,  ..., -0.8748, -0.2904,  0.2541],\n",
       "           [-0.2260,  1.0642,  1.8526,  ...,  0.1167,  0.5125,  0.0517]],\n",
       " \n",
       "          [[ 1.1500,  2.8287,  3.6016,  ...,  0.4104, -1.0314, -0.5081],\n",
       "           [ 1.6337,  3.2925,  4.0578,  ...,  0.4298,  0.3292,  0.6380],\n",
       "           [ 1.7015,  3.5075,  4.2591,  ...,  0.5702,  0.1451, -0.9440],\n",
       "           ...,\n",
       "           [-1.6427,  1.7337,  2.5271,  ...,  0.0628,  0.2261,  0.3994],\n",
       "           [ 0.1533,  1.7749,  2.3512,  ..., -0.1578,  0.4130,  0.7051],\n",
       "           [-0.3814,  1.8864,  2.5896,  ...,  0.1996,  0.4434,  0.6492]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 0.8552,  1.3230,  2.0196,  ..., -0.5518, -1.2387, -0.7896],\n",
       "           [ 0.7011,  1.6405,  2.0757,  ...,  0.1213,  0.3821,  0.5097],\n",
       "           [ 0.1861,  1.8341,  2.2903,  ..., -0.2413,  0.1880, -0.0878],\n",
       "           ...,\n",
       "           [ 1.1582,  1.4819,  1.4647,  ..., -0.3947, -0.4217, -0.2942],\n",
       "           [ 1.0504,  0.9793,  1.0217,  ..., -0.0358, -0.2121, -0.4640],\n",
       "           [ 1.1900,  1.4384,  1.0814,  ..., -0.1612, -0.5016, -0.4238]],\n",
       " \n",
       "          [[ 0.9574,  3.1622,  3.3618,  ...,  0.3255, -0.2604,  0.0339],\n",
       "           [ 1.2515,  3.2116,  3.4137,  ...,  0.6878,  0.3404, -0.1676],\n",
       "           [ 1.0305,  3.1604,  3.3621,  ...,  0.5209,  0.2921,  0.3696],\n",
       "           ...,\n",
       "           [-0.3661,  1.5398,  0.4254,  ...,  0.5995, -0.0598, -0.7220],\n",
       "           [ 0.2592,  2.2378,  2.6001,  ...,  0.5686,  0.7133,  0.2845],\n",
       "           [ 0.0516,  1.7156,  2.1851,  ..., -0.3434,  0.1634,  0.4709]]],\n",
       " \n",
       " \n",
       "         [[[-0.3478,  1.8195,  2.1647,  ..., -0.0987, -0.1867,  0.0718],\n",
       "           [-0.2625,  1.9083,  2.3400,  ..., -0.2294,  0.0382, -0.0286],\n",
       "           [ 0.8969,  2.0357,  2.4605,  ...,  0.5022, -0.0568,  0.0506],\n",
       "           ...,\n",
       "           [ 0.6139,  1.5826,  1.9807,  ..., -1.3222, -0.2017, -0.3425],\n",
       "           [ 0.4696,  1.1124,  1.4055,  ..., -0.5635, -0.1984, -1.6145],\n",
       "           [ 1.1504,  1.1633,  1.1237,  ..., -0.0309, -0.1796, -0.4845]],\n",
       " \n",
       "          [[ 2.0574,  3.1045,  3.3718,  ...,  0.4623,  0.4290,  0.3149],\n",
       "           [ 2.0385,  3.1676,  3.4539,  ..., -0.1323, -0.3042, -1.0001],\n",
       "           [ 2.0160,  3.0896,  3.3725,  ...,  0.2709, -0.1982,  0.0275],\n",
       "           ...,\n",
       "           [ 0.3135,  1.6218,  2.4261,  ..., -0.1577,  0.0112, -0.3183],\n",
       "           [-0.5162,  2.0395,  2.4513,  ...,  0.4347,  0.3798, -0.6214],\n",
       "           [ 0.2812,  1.7571,  2.1867,  ..., -1.2444, -0.2787,  0.2488]]],\n",
       " \n",
       " \n",
       "         [[[ 1.3174,  1.8326,  1.9835,  ..., -0.3371, -0.1995,  0.4567],\n",
       "           [ 1.2714,  1.8230,  1.9281,  ..., -1.5200, -1.1391, -1.1042],\n",
       "           [ 1.2594,  1.9493,  1.8439,  ...,  0.6303, -0.0123,  0.0342],\n",
       "           ...,\n",
       "           [ 0.4093,  1.2887,  1.4238,  ..., -0.1892, -0.6252, -4.2372],\n",
       "           [ 0.7044,  0.9481, -0.5549,  ..., -0.9532, -0.5717, -0.2303],\n",
       "           [ 0.7849,  1.3066,  0.6938,  ...,  0.3780,  0.4582,  0.2503]],\n",
       " \n",
       "          [[ 2.1034,  2.7448,  2.9029,  ...,  0.4581, -1.2958, -0.1931],\n",
       "           [ 2.1422,  2.8228,  3.0083,  ..., -1.0575, -1.3530, -1.0197],\n",
       "           [ 2.0567,  2.6975,  2.9218,  ..., -0.4109, -0.6644, -2.5651],\n",
       "           ...,\n",
       "           [ 1.0305,  2.0547,  2.5780,  ..., -0.9175, -0.7888, -2.0756],\n",
       "           [ 0.6442,  2.1434,  2.4328,  ...,  0.1639,  0.3365, -0.5561],\n",
       "           [ 0.8180,  2.0182,  2.2494,  ..., -0.4821, -1.2578, -0.0579]]]]),\n",
       " tensor([95], dtype=torch.int32))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_data.__getitem__(4999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emg2qwerty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
